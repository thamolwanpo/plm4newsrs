# Dataset configuration
dataset: "politifact"
experiment_name: "simple_model"
model_type: "clean"

# Model architecture
model_name: "roberta-base"
use_pretrained_lm: true
fine_tune_lm: true

# Architecture hyperparameters
max_seq_length: 50
max_history_length: 5
num_attention_heads: 16
news_query_vector_dim: 200
user_query_vector_dim: 200
drop_rate: 0.2

# Training hyperparameters
epochs: 10
train_batch_size: 16
val_batch_size: 32
learning_rate: 1.0e-5  # RoBERTa often needs lower LR
early_stopping_patience: 3

# Paths
base_dir: null
models_dir: null

# Reproducibility
seed: 42

# Additional training options
gradient_clip_val: 1.0
warmup_steps: 0