# Dataset configuration
dataset: "politifact"
experiment_name: "simple_model"
model_type: "clean"

# Model architecture
model_name: "bert-base-uncased"
use_pretrained_lm: true
fine_tune_lm: true

# Architecture hyperparameters
max_seq_length: 50
max_history_length: 5
num_attention_heads: 16
news_query_vector_dim: 200
user_query_vector_dim: 200
drop_rate: 0.2

# Training hyperparameters
epochs: 10
train_batch_size: 16
val_batch_size: 32
learning_rate: 2.0e-5  # Lower LR for fine-tuning
early_stopping_patience: 3

# Paths (can be overridden)
base_dir: null  # Will use default: ./{dataset}
models_dir: null  # Will use default: {base_dir}/{experiment_name}/models

# Reproducibility
seed: 42

# Additional training options
gradient_clip_val: 1.0
warmup_steps: 0