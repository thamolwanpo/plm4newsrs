# Dataset configuration
dataset: "politifact"
experiment_name: "simple_model"
model_type: "clean"

# Model architecture
model_name: "bert-base-uncased"
use_pretrained_lm: true
fine_tune_lm: false  # Frozen!

# Architecture hyperparameters
max_seq_length: 50
max_history_length: 5
num_attention_heads: 16
news_query_vector_dim: 200
user_query_vector_dim: 200
drop_rate: 0.2

# Training hyperparameters
epochs: 15  # More epochs since we're only training attention
train_batch_size: 32  # Larger batch since less memory needed
val_batch_size: 64
learning_rate: 1.0e-3  # Higher LR for frozen backbone
early_stopping_patience: 3

# Paths
base_dir: null
models_dir: null

# Reproducibility
seed: 42

# Additional training options
gradient_clip_val: 1.0
warmup_steps: 0