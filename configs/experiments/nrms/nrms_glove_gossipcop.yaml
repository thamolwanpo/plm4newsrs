# Dataset configuration
dataset: "gossipcop"
experiment_name: "nrms_model"
model_type: "clean"

# Architecture
architecture: "nrms"

# Model configuration
model_name: "glove"
use_pretrained_lm: true
fine_tune_lm: false

# GloVe-specific settings
glove_file_path: "lm_models/glove.42B.300d.txt"
glove_model: "glove-42B-300"
glove_dim: 300
glove_aggregation: "attention"

# Architecture hyperparameters
max_seq_length: 50
max_history_length: 50

# Multi-head self-attention
# 300 is divisible by: 1, 2, 3, 4, 5, 6, 10, 12, 15, 20, 25, 30, 50, 60, 75, 100, 150, 300
num_attention_heads: 12  # Changed from 16 to 12 (300 / 12 = 25)
attention_hidden_dim: 200
num_user_attention_heads: 12  # Changed from 16 to 12

drop_rate: 0.2

# Training hyperparameters
epochs: 20
train_batch_size: 32
val_batch_size: 64
learning_rate: 1.0e-3
early_stopping_patience: 5

# Paths
base_dir: null
models_dir: null

# Reproducibility
seed: 42

# Additional training options
gradient_clip_val: 1.0
warmup_steps: 0